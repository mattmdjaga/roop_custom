{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py --source ted.jpeg --target van.jpeg --output roop.jpeg --execution-provider cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roop.globals\n",
    "import roop.ui as ui\n",
    "import shutil\n",
    "from roop.processors.frame.core import get_frame_processors_modules\n",
    "from roop.utilities import normalize_output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "def encode_execution_providers(execution_providers):\n",
    "    return [execution_provider.replace('ExecutionProvider', '').lower() for execution_provider in execution_providers]\n",
    "\n",
    "def decode_execution_providers(execution_providers):\n",
    "    return [provider for provider, encoded_execution_provider in zip(onnxruntime.get_available_providers(), encode_execution_providers(onnxruntime.get_available_providers()))\n",
    "        if any(execution_provider in encoded_execution_provider for execution_provider in execution_providers)]\n",
    "\n",
    "def suggest_execution_providers():\n",
    "    return encode_execution_providers(onnxruntime.get_available_providers())\n",
    "\n",
    "def suggest_execution_threads() -> int:\n",
    "    if 'CUDAExecutionProvider' in onnxruntime.get_available_providers():\n",
    "        return 8\n",
    "    return 1\n",
    "\n",
    "def update_status(message: str, scope: str = 'ROOP.CORE') -> None:\n",
    "    print(f'[{scope}] {message}')\n",
    "    if not roop.globals.headless:\n",
    "        ui.update_status(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roop.utilities import normalize_output_path\n",
    "# Default values taken from program.add_argument where provided\n",
    "roop.globals.source_path = \"ted.jpeg\"  # default Python value for not specified\n",
    "roop.globals.target_path = \"van.jpeg\"  # default Python value for not specified\n",
    "roop.globals.output_path = None  # Assuming normalization handles None values\n",
    "roop.globals.headless = None is not None and None is not None and None is not None  # Resulting in False\n",
    "roop.globals.frame_processors = ['face_swapper']  # Default provided\n",
    "roop.globals.keep_fps = False  # Default for store_true action is False\n",
    "roop.globals.keep_frames = False  # Default for store_true action is False\n",
    "roop.globals.skip_audio = False  # Default for store_true action is False\n",
    "roop.globals.many_faces = False  # Default for store_true action is False\n",
    "roop.globals.reference_face_position = 0  # Default provided\n",
    "roop.globals.reference_frame_number = 0  # Default provided\n",
    "roop.globals.similar_face_distance = 0.85  # Default provided\n",
    "roop.globals.temp_frame_format = 'png'  # Default provided\n",
    "roop.globals.temp_frame_quality = 0  # Default provided\n",
    "roop.globals.output_video_encoder = 'libx264'  # Default provided\n",
    "roop.globals.output_video_quality = 35  # Default provided\n",
    "roop.globals.max_memory = None  # No default provided in add_argument\n",
    "roop.globals.execution_providers = decode_execution_providers(['cpu'])  # Default provided\n",
    "roop.globals.execution_threads = suggest_execution_threads()  # Default provided dynamically\n",
    "roop.globals.headless = True  # Default for store_true action is False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any, Tuple\n",
    "import io\n",
    "import base64\n",
    "import sys\n",
    "sys.path.append('CodeFormer/CodeFormer')\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torchvision.transforms.functional import normalize\n",
    "\n",
    "from basicsr.utils import imwrite, img2tensor, tensor2img\n",
    "from basicsr.utils.download_util import load_file_from_url\n",
    "from facelib.utils.face_restoration_helper import FaceRestoreHelper\n",
    "from facelib.utils.misc import is_gray\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from basicsr.utils.realesrgan_utils import RealESRGANer\n",
    "\n",
    "from basicsr.utils.registry import ARCH_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model_url = {\n",
    "    'codeformer': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth',\n",
    "    'detection': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/detection_Resnet50_Final.pth',\n",
    "    'parsing': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/parsing_parsenet.pth',\n",
    "    'realesrgan': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/RealESRGAN_x2plus.pth'\n",
    "}\n",
    "# download weights\n",
    "if not os.path.exists('CodeFormer/CodeFormer/weights/CodeFormer/codeformer.pth'):\n",
    "    load_file_from_url(url=pretrain_model_url['codeformer'], model_dir='CodeFormer/CodeFormer/weights/CodeFormer', progress=True, file_name=None)\n",
    "if not os.path.exists('/CodeFormer/CodeFormer/weights/facelib/detection_Resnet50_Final.pth'):\n",
    "    load_file_from_url(url=pretrain_model_url['detection'], model_dir='CodeFormer/CodeFormer/weights/facelib', progress=True, file_name=None)\n",
    "if not os.path.exists('CodeFormer/CodeFormer/weights/facelib/parsing_parsenet.pth'):\n",
    "    load_file_from_url(url=pretrain_model_url['parsing'], model_dir='CodeFormer/CodeFormer/weights/facelib', progress=True, file_name=None)\n",
    "if not os.path.exists('CodeFormer/CodeFormer/weights/realesrgan/RealESRGAN_x2plus.pth'):\n",
    "    load_file_from_url(url=pretrain_model_url['realesrgan'], model_dir='CodeFormer/CodeFormer/weights/realesrgan', progress=True, file_name=None)\n",
    "\n",
    "\n",
    "def imread(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "# set enhancer with RealESRGAN\n",
    "def set_realesrgan():\n",
    "    half = True if torch.cuda.is_available() else False\n",
    "    model = RRDBNet(\n",
    "        num_in_ch=3,\n",
    "        num_out_ch=3,\n",
    "        num_feat=64,\n",
    "        num_block=23,\n",
    "        num_grow_ch=32,\n",
    "        scale=2,\n",
    "    )\n",
    "    upsampler = RealESRGANer(\n",
    "        scale=2,\n",
    "        model_path=\"CodeFormer/CodeFormer/weights/realesrgan/RealESRGAN_x2plus.pth\",\n",
    "        model=model,\n",
    "        tile=400,\n",
    "        tile_pad=40,\n",
    "        pre_pad=0,\n",
    "        half=half,\n",
    "    )\n",
    "    return upsampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampler = set_realesrgan()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "codeformer_net = ARCH_REGISTRY.get(\"CodeFormer\")(\n",
    "    dim_embd=512,\n",
    "    codebook_size=1024,\n",
    "    n_head=8,\n",
    "    n_layers=9,\n",
    "    connect_list=[\"32\", \"64\", \"128\", \"256\"],\n",
    ").to(device)\n",
    "ckpt_path = \"CodeFormer/CodeFormer/weights/CodeFormer/codeformer.pth\"\n",
    "checkpoint = torch.load(ckpt_path)[\"params_ema\"]\n",
    "codeformer_net.load_state_dict(checkpoint)\n",
    "codeformer_net.eval()\n",
    "\n",
    "#os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(args):\n",
    "    \"\"\"Run a single prediction on the model\"\"\"\n",
    "    image, face_align, background_enhance, face_upsample, upscale, codeformer_fidelity = args\n",
    "    try: # global try\n",
    "        # take the default setting for the demo\n",
    "        only_center_face = False\n",
    "        draw_box = False\n",
    "        detection_model = \"retinaface_resnet50\"\n",
    "\n",
    "        #print('Inp:', image, background_enhance, face_upsample, upscale, codeformer_fidelity)\n",
    "        face_align = face_align if face_align is not None else True\n",
    "        background_enhance = background_enhance if background_enhance is not None else True\n",
    "        face_upsample = face_upsample if face_upsample is not None else True\n",
    "        upscale = upscale if (upscale is not None and upscale > 0) else 2\n",
    "\n",
    "        has_aligned = not face_align\n",
    "        upscale = 1 if has_aligned else upscale\n",
    "\n",
    "        #img = cv2.imread(str(image), cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        #print('\\timage size:', img.shape)\n",
    "\n",
    "        upscale = int(upscale) # convert type to int\n",
    "        if upscale > 4: # avoid memory exceeded due to too large upscale\n",
    "            upscale = 4 \n",
    "        if upscale > 2 and max(img.shape[:2])>1000: # avoid memory exceeded due to too large img resolution\n",
    "            upscale = 2 \n",
    "        if max(img.shape[:2]) > 1500: # avoid memory exceeded due to too large img resolution\n",
    "            upscale = 1\n",
    "            background_enhance = False\n",
    "            face_upsample = False\n",
    "\n",
    "        face_helper = FaceRestoreHelper(\n",
    "            upscale,\n",
    "            face_size=512,\n",
    "            crop_ratio=(1, 1),\n",
    "            det_model=detection_model,\n",
    "            save_ext=\"png\",\n",
    "            use_parse=True,\n",
    "            device=device,\n",
    "        )\n",
    "        bg_upsampler = upsampler if background_enhance else None\n",
    "        face_upsampler = upsampler if face_upsample else None\n",
    "\n",
    "        if has_aligned:\n",
    "            # the input faces are already cropped and aligned\n",
    "            img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
    "            face_helper.is_gray = is_gray(img, threshold=5)\n",
    "            if face_helper.is_gray:\n",
    "                #print('\\tgrayscale input: True')\n",
    "                xyz = 5\n",
    "            face_helper.cropped_faces = [img]\n",
    "        else:\n",
    "            face_helper.read_image(img)\n",
    "            # get face landmarks for each face\n",
    "            num_det_faces = face_helper.get_face_landmarks_5(\n",
    "            only_center_face=only_center_face, resize=640, eye_dist_threshold=5\n",
    "            )\n",
    "            #print(f'\\tdetect {num_det_faces} faces')\n",
    "            # align and warp each face\n",
    "            face_helper.align_warp_face()\n",
    "\n",
    "        # face restoration for each cropped face\n",
    "        for idx, cropped_face in enumerate(face_helper.cropped_faces):\n",
    "            # prepare data\n",
    "            cropped_face_t = img2tensor(\n",
    "                cropped_face / 255.0, bgr2rgb=True, float32=True\n",
    "            )\n",
    "            normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
    "            cropped_face_t = cropped_face_t.unsqueeze(0).to(device)\n",
    "\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    output = codeformer_net(\n",
    "                        cropped_face_t, w=codeformer_fidelity, adain=True\n",
    "                    )[0]\n",
    "                    restored_face = tensor2img(output, rgb2bgr=True, min_max=(-1, 1))\n",
    "                del output\n",
    "                torch.cuda.empty_cache()\n",
    "            except RuntimeError as error:\n",
    "                #print(f\"Failed inference for CodeFormer: {error}\")\n",
    "                restored_face = tensor2img(\n",
    "                    cropped_face_t, rgb2bgr=True, min_max=(-1, 1)\n",
    "                )\n",
    "\n",
    "            restored_face = restored_face.astype(\"uint8\")\n",
    "            face_helper.add_restored_face(restored_face)\n",
    "\n",
    "        # paste_back\n",
    "        if not has_aligned:\n",
    "            # upsample the background\n",
    "            if bg_upsampler is not None:\n",
    "                # Now only support RealESRGAN for upsampling background\n",
    "                bg_img = bg_upsampler.enhance(img, outscale=upscale)[0]\n",
    "            else:\n",
    "                bg_img = None\n",
    "            face_helper.get_inverse_affine(None)\n",
    "            # paste each restored face to the input image\n",
    "            if face_upsample and face_upsampler is not None:\n",
    "                restored_img = face_helper.paste_faces_to_input_image(\n",
    "                    upsample_img=bg_img,\n",
    "                    draw_box=draw_box,\n",
    "                    face_upsampler=face_upsampler,\n",
    "                )\n",
    "            else:\n",
    "                restored_img = face_helper.paste_faces_to_input_image(\n",
    "                    upsample_img=bg_img, draw_box=draw_box\n",
    "                )\n",
    "        else:\n",
    "            restored_img = restored_face\n",
    "\n",
    "        # save restored img\n",
    "        save_path = f'output/out.png'\n",
    "        imwrite(restored_img, str(save_path))\n",
    "\n",
    "        restored_img = cv2.cvtColor(restored_img, cv2.COLOR_BGR2RGB)\n",
    "        return restored_img\n",
    "    except Exception as error:\n",
    "        print('Global exception', error)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image: Image, img_format: int = \"JPEG\") -> str:\n",
    "    \"\"\"\n",
    "    Encodes image to base64 string.\n",
    "    Args:\n",
    "        image: PIL image\n",
    "        img_format: image format\n",
    "    Returns:\n",
    "        base64 string\n",
    "    \"\"\"\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    image.save(img_byte_arr, format=img_format)\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "    img_byte_arr = base64.b64encode(img_byte_arr).decode(\"utf-8\")\n",
    "    return img_byte_arr\n",
    "\n",
    "\n",
    "def decode_image(image: str) -> Image:\n",
    "    \"\"\"\n",
    "    Decodes base64 string to PIL image.\n",
    "    Args:\n",
    "        image: base64 string\n",
    "    Returns:\n",
    "        PIL image\n",
    "    \"\"\"\n",
    "    img_byte_arr = base64.b64decode(image)\n",
    "    img_byte_arr = io.BytesIO(img_byte_arr)\n",
    "    img_byte_arr = Image.open(img_byte_arr)\n",
    "    return img_byte_arr\n",
    "\n",
    "def image_grid(imgs: List[Any], rows: int = 2, cols: int = 2):\n",
    "    \"\"\"\n",
    "    Creates a grid of images.\n",
    "    Args:\n",
    "        imgs: list of PIL images\n",
    "        rows: number of rows\n",
    "        cols: number of columns\n",
    "    Returns:\n",
    "        PIL image\n",
    "    \"\"\"\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"inputs\"\n",
    "bases_folder = \"bases\"\n",
    "input_imgs = [f\"{input_folder}/{img}\" for img in os.listdir(input_folder)]\n",
    "base_imgs = [f\"{bases_folder}/{img}\" for img in os.listdir(bases_folder)]\n",
    "input_imgs.sort()\n",
    "base_imgs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roop_api(source_image, target_image):\n",
    "    \"\"\"\n",
    "    Swaps faces in source image with faces in target image.\n",
    "    Args:\n",
    "        source_image: source image\n",
    "        target_image: target image\n",
    "    Returns:\n",
    "        swapped image\n",
    "    \"\"\"\n",
    "    address = \"http://gamma.pons.ai:1861\"\n",
    "    source_base64 = encode_image(Image.open(source_image))\n",
    "    target_base64 = encode_image(Image.open(target_image))\n",
    "    payload = {\n",
    "        \"source_image\": source_base64,\n",
    "        \"target_image\": target_base64,\n",
    "        \"face_index\": [0],\n",
    "        \"scale\": 1,\n",
    "        \"upscale_visibility\": 1,\n",
    "        \"face_restorer\": \"CodeFormer\",\n",
    "        \"restorer_visibility\": 1,\n",
    "        \"model\": \"inswapper_128.onnx\",\n",
    "    }\n",
    "    out = requests.post(url=f\"{address}/roop/image\", json=payload)\n",
    "    gen_image = decode_image(out.json()[\"image\"])\n",
    "    return gen_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from matplotlib import pyplot as plt\n",
    "import requests\n",
    "api_imgs = []\n",
    "for i in range(4):\n",
    "    in_img = input_imgs[i]\n",
    "    base_img = base_imgs[i]\n",
    "    gen_img = roop_api(in_img, base_img)\n",
    "    api_imgs.append(gen_img)\n",
    "grid = image_grid(api_imgs, cols=len(api_imgs)//2, rows=2)\n",
    "plt.figure(figsize=(70, 10))\n",
    "plt.imshow(grid)\n",
    "plt.show()\n",
    "#grid.save(\"api_grid.jpeg\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import concurrent.futures\n",
    "imgs = []\n",
    "args_roop = []\n",
    "for i in range(4):\n",
    "    in_img = Image.open(input_imgs[i])\n",
    "    base_img = Image.open(base_imgs[i])\n",
    "    arg_roop = (in_img, base_img)\n",
    "    args_roop.append(arg_roop)\n",
    "\n",
    "frame_processors = [\"face_swapper\"]\n",
    "#shutil.copy2(base_img, roop.globals.output_path)\n",
    "for frame_processor in get_frame_processors_modules(frame_processors):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(frame_processor.process_image, args_roop))\n",
    "\n",
    "imgs = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in results]\n",
    "\n",
    "args_codeformer = [(img, True, False, False, 0, 1.0) for img in imgs]\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    results = list(executor.map(inference, args_codeformer))\n",
    "imgs = [Image.fromarray(img) for img in results]\n",
    "grid = image_grid(imgs, cols=len(imgs)//2, rows=2)\n",
    "plt.figure(figsize=(70, 10))\n",
    "plt.imshow(grid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
